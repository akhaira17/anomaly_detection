{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly_config.py\n",
    "import os\n",
    "\n",
    "CONFIG = {\n",
    "    # \"DB_CONNECTION_STRING\": os.getenv(\"DB_CONNECTION_STRING\", \"sqlite:///trades.db\"),  # Removed or commented out\n",
    "    \"BATCH_DATE\": os.getenv(\"BATCH_DATE\", \"2024-12-11\"),\n",
    "    \"EIF_PARAMS\": {\n",
    "        \"ntrees\": 100,\n",
    "        \"sample_size\": 256,\n",
    "        \"extension_level\": 1,  # Corrected to match 'ExtensionLevel' in eif\n",
    "        \"contamination\": 0.01  # 1% anomalies\n",
    "    },\n",
    "    \"DEEPIF_PARAMS\": {\n",
    "        \"latent_dim\": 2,\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"if_n_estimators\": 100,\n",
    "        \"if_contamination\": 0.01\n",
    "    },\n",
    "    \"FEATURE_COLUMNS\": [\"our_cents\", \"cp_cents\", \"notional\", \"impact_dollars\", \"product_type\", \"counterparty\"],\n",
    "    \"LOG_LEVEL\": \"INFO\",\n",
    "    \"RESULT_TABLE\": \"trade_anomaly_detection_results\",  # Can be removed if not using\n",
    "    \"SAMPLE_DATA_SIZE\": 1000  # Number of samples for synthetic data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_generation.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_synthetic_data(size=1000, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Numerical features\n",
    "    our_cents = np.random.normal(loc=100, scale=10, size=size)\n",
    "    cp_cents = our_cents + np.random.normal(loc=0, scale=5, size=size)  # Slight variations\n",
    "    \n",
    "    # Introduce anomalies\n",
    "    n_anomalies = int(size * 0.01)  # 1% anomalies\n",
    "    anomaly_indices = np.random.choice(size, n_anomalies, replace=False)\n",
    "    our_cents[anomaly_indices] += np.random.normal(loc=50, scale=10, size=n_anomalies)  # Large deviation\n",
    "    \n",
    "    notional = np.random.uniform(10000, 1000000, size=size)\n",
    "    impact_dollars = np.abs(our_cents - cp_cents) * notional / 100.0\n",
    "    \n",
    "    # Categorical features\n",
    "    product_types = ['swaption', 'vanilla_swap', 'option', 'forward']\n",
    "    counterparties = ['CP_A', 'CP_B', 'CP_C', 'CP_D']\n",
    "    \n",
    "    product_type = np.random.choice(product_types, size=size)\n",
    "    counterparty = np.random.choice(counterparties, size=size)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'trade_id': range(1, size + 1),\n",
    "        'our_cents': our_cents,\n",
    "        'cp_cents': cp_cents,\n",
    "        'notional': notional,\n",
    "        'impact_dollars': impact_dollars,\n",
    "        'product_type': product_type,\n",
    "        'counterparty': counterparty\n",
    "    })\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocessing.py\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def preprocess_data(df, feature_columns):\n",
    "    # Separate features\n",
    "    X = df[feature_columns]\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_cols = ['product_type', 'counterparty']\n",
    "    numerical_cols = ['our_cents', 'cp_cents', 'notional', 'impact_dollars']\n",
    "    \n",
    "    # Preprocessing pipelines\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Get feature names after encoding\n",
    "    ohe = preprocessor.named_transformers_['cat']['onehot']\n",
    "    cat_features = ohe.get_feature_names_out(categorical_cols)\n",
    "    all_features = numerical_cols + list(cat_features)\n",
    "    \n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=all_features)\n",
    "    \n",
    "    return X_processed_df, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eif'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meif\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iForest\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_eif\u001b[39m(df, feature_columns, eif_params):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eif'"
     ]
    }
   ],
   "source": [
    "# eif_model.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from eif import iForest\n",
    "import logging\n",
    "\n",
    "def run_eif(df, feature_columns, eif_params):\n",
    "    \"\"\"\n",
    "    Apply Extended Isolation Forest (EIF) for anomaly detection.\n",
    "    \"\"\"\n",
    "    # Log available columns\n",
    "    logging.debug(f\"Available columns in EIF DataFrame: {df.columns.tolist()}\")\n",
    "    logging.debug(f\"Expected feature columns: {feature_columns}\")\n",
    "    \n",
    "    # Extract features\n",
    "    X = df[feature_columns].values\n",
    "    \n",
    "    # Initialize EIF with corrected parameter names and pass X as the first argument\n",
    "    try:\n",
    "        model = iForest(\n",
    "            X,  # Pass X as the first positional argument\n",
    "            ntrees=eif_params['ntrees'],\n",
    "            sample_size=eif_params['sample_size'],\n",
    "            ExtensionLevel=eif_params['extension_level']  # Corrected parameter name\n",
    "        )\n",
    "    except TypeError as te:\n",
    "        logging.error(f\"TypeError during EIF model initialization: {te}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during EIF model initialization: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Compute anomaly scores (lower scores are more anomalous)\n",
    "    try:\n",
    "        scores = model.compute_paths(X_in=X)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error computing anomaly scores: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Determine threshold based on contamination\n",
    "    try:\n",
    "        threshold = np.percentile(scores, eif_params['contamination'] * 100)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating threshold: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Label anomalies\n",
    "    labels = scores < threshold  # True if anomaly\n",
    "    \n",
    "    # Add to DataFrame\n",
    "    df['eif_anomaly_label'] = labels\n",
    "    df['eif_anomaly_score'] = scores\n",
    "    \n",
    "    logging.info(f\"EIF: Detected {labels.sum()} anomalies out of {len(labels)} trades.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepif_model.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "import shap\n",
    "\n",
    "# Define the Autoencoder in PyTorch\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_dim)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed, latent\n",
    "\n",
    "def train_autoencoder(model, dataloader, epochs, lr, device):\n",
    "    \"\"\"\n",
    "    Train the autoencoder model.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            data = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, _ = model(data)\n",
    "            loss = criterion(reconstructed, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        logging.info(f\"DeepIF: Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_deepif(df, feature_columns, deepif_params, preprocessor):\n",
    "    \"\"\"\n",
    "    Apply Deep Isolation Forest (DeepIF) for anomaly detection.\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    X = df[feature_columns].values\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=deepif_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Define autoencoder\n",
    "    input_dim = X_scaled.shape[1]\n",
    "    latent_dim = deepif_params['latent_dim']\n",
    "    autoencoder = Autoencoder(input_dim, latent_dim)\n",
    "    \n",
    "    # Train autoencoder\n",
    "    autoencoder = train_autoencoder(\n",
    "        model=autoencoder,\n",
    "        dataloader=dataloader,\n",
    "        epochs=deepif_params['epochs'],\n",
    "        lr=deepif_params['learning_rate'],\n",
    "        device='cpu'  # Change to 'cuda' if GPU is available\n",
    "    )\n",
    "    \n",
    "    # Extract latent embeddings\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_embeddings = autoencoder.encoder(X_tensor)  # Corrected line\n",
    "    latent_embeddings = latent_embeddings.numpy()\n",
    "    \n",
    "    # Train Isolation Forest on latent embeddings\n",
    "    isolation_forest = IsolationForest(\n",
    "        n_estimators=deepif_params['if_n_estimators'],\n",
    "        contamination=deepif_params['if_contamination'],\n",
    "        random_state=42\n",
    "    )\n",
    "    isolation_forest.fit(latent_embeddings)\n",
    "    \n",
    "    # Predict anomalies\n",
    "    anomaly_scores = isolation_forest.decision_function(latent_embeddings)\n",
    "    anomaly_labels = isolation_forest.predict(latent_embeddings)  # -1 = anomaly, 1 = normal\n",
    "    \n",
    "    # Add results to DataFrame\n",
    "    df['deepif_anomaly_score'] = anomaly_scores\n",
    "    df['deepif_anomaly_label'] = (anomaly_labels == -1)\n",
    "    \n",
    "    logging.info(f\"DeepIF: Detected {df['deepif_anomaly_label'].sum()} anomalies out of {len(df)} trades.\")\n",
    "    \n",
    "    # Optional: Interpretability with SHAP\n",
    "    # Note: SHAP explanations for autoencoder can be complex. Here's a basic example.\n",
    "    try:\n",
    "        explainer = shap.Explainer(autoencoder.encoder, X_scaled)\n",
    "        shap_values = explainer(X_scaled[:100])  # Explain first 100 instances for speed\n",
    "        shap.summary_plot(shap_values, X_scaled[:100], feature_names=preprocessor.get_feature_names_out())\n",
    "    except Exception as e:\n",
    "        logging.error(f\"SHAP explainability failed: {e}\")\n",
    "    \n",
    "    return df, isolation_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_results.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def combine_results(df_eif, df_deepif, original_df):\n",
    "    \"\"\"\n",
    "    Combine EIF and DeepIF results with the original DataFrame.\n",
    "    Strategies:\n",
    "    - Logical AND: Flag as anomaly only if both models agree.\n",
    "    - Average Scores: Combine anomaly scores for ranking.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrames have the same index\n",
    "    if not (df_eif.index.equals(df_deepif.index) and df_eif.index.equals(original_df.index)):\n",
    "        logging.error(\"DataFrames do not have matching indices. Cannot combine results.\")\n",
    "        raise ValueError(\"DataFrames indices mismatch.\")\n",
    "    \n",
    "    # Merge anomaly labels and scores\n",
    "    combined_df = original_df.copy()\n",
    "    combined_df['eif_anomaly_label'] = df_eif['eif_anomaly_label']\n",
    "    combined_df['eif_anomaly_score'] = df_eif['eif_anomaly_score']\n",
    "    combined_df['deepif_anomaly_label'] = df_deepif['deepif_anomaly_label']\n",
    "    combined_df['deepif_anomaly_score'] = df_deepif['deepif_anomaly_score']\n",
    "    \n",
    "    # Logical AND\n",
    "    combined_df['combined_anomaly_label'] = combined_df['eif_anomaly_label'] & combined_df['deepif_anomaly_label']\n",
    "    \n",
    "    # Average Scores (assuming lower scores indicate anomalies)\n",
    "    combined_df['combined_anomaly_score'] = (combined_df['eif_anomaly_score'] + combined_df['deepif_anomaly_score']) / 2.0\n",
    "    \n",
    "    logging.info(f\"Combined Results: {combined_df['combined_anomaly_label'].sum()} anomalies detected.\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretability.py\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explain_anomalies(model, df, feature_columns, preprocessor, top_n=10):\n",
    "    \"\"\"\n",
    "    Use SHAP to explain the top N anomalies.\n",
    "    \"\"\"\n",
    "    # Select top N anomalies based on combined score\n",
    "    anomalies = df[df['combined_anomaly_label']].sort_values('combined_anomaly_score').head(top_n)\n",
    "    if anomalies.empty:\n",
    "        print(\"No anomalies to explain.\")\n",
    "        return\n",
    "    \n",
    "    # Preprocess the data as done before\n",
    "    X = anomalies[feature_columns].values\n",
    "    X_processed = preprocessor.transform(anomalies[feature_columns])\n",
    "    \n",
    "    # Create a DataFrame with processed features\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    X_processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "    \n",
    "    # Initialize SHAP\n",
    "    explainer = shap.Explainer(model.encoder, X_processed_df)\n",
    "    shap_values = explainer(X_processed_df)\n",
    "    \n",
    "    # Plot SHAP summary\n",
    "    shap.summary_plot(shap_values, X_processed_df, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_results.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def store_results(df, config):\n",
    "    \"\"\"\n",
    "    Store anomaly detection results by exporting to a CSV file.\n",
    "    \"\"\"\n",
    "    # Define the output CSV file path\n",
    "    output_file = 'trade_anomaly_detection_results.csv'\n",
    "    \n",
    "    try:\n",
    "        # Export the DataFrame to CSV\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Results successfully exported to '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to export results to CSV: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 12:23:17,107 [INFO] Generating synthetic data...\n",
      "2024-12-12 12:23:17,112 [INFO] Preprocessing data...\n",
      "2024-12-12 12:23:17,117 [INFO] Running Extended Isolation Forest (EIF)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   trade_id   our_cents    cp_cents       notional  impact_dollars  \\\n",
      "0         1  104.967142  111.963919   31920.060718     2233.375525   \n",
      "1         2   98.617357  103.240525  945663.046868    43719.595291   \n",
      "2         3  106.476885  106.775037  699660.998505     2086.052208   \n",
      "3         4  115.230299  111.995615  500746.712730    16197.573239   \n",
      "4         5   97.658466  101.149583  933795.962064    32599.905544   \n",
      "\n",
      "   product_type counterparty  \n",
      "0      swaption         CP_B  \n",
      "1  vanilla_swap         CP_A  \n",
      "2      swaption         CP_A  \n",
      "3  vanilla_swap         CP_A  \n",
      "4      swaption         CP_C  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 12:23:18,008 [INFO] EIF: Detected 10 anomalies out of 1000 trades.\n",
      "2024-12-12 12:23:18,013 [INFO] Running Deep Isolation Forest (DeepIF)...\n",
      "2024-12-12 12:23:18,031 [INFO] DeepIF: Epoch 1/50, Loss: 38.1730\n",
      "2024-12-12 12:23:18,048 [INFO] DeepIF: Epoch 2/50, Loss: 33.2384\n",
      "2024-12-12 12:23:18,064 [INFO] DeepIF: Epoch 3/50, Loss: 31.6126\n",
      "2024-12-12 12:23:18,081 [INFO] DeepIF: Epoch 4/50, Loss: 31.0320\n",
      "2024-12-12 12:23:18,098 [INFO] DeepIF: Epoch 5/50, Loss: 30.7357\n",
      "2024-12-12 12:23:18,115 [INFO] DeepIF: Epoch 6/50, Loss: 28.0840\n",
      "2024-12-12 12:23:18,131 [INFO] DeepIF: Epoch 7/50, Loss: 26.9228\n",
      "2024-12-12 12:23:18,162 [INFO] DeepIF: Epoch 8/50, Loss: 26.4190\n",
      "2024-12-12 12:23:18,178 [INFO] DeepIF: Epoch 9/50, Loss: 26.0102\n",
      "2024-12-12 12:23:18,193 [INFO] DeepIF: Epoch 10/50, Loss: 25.8654\n",
      "2024-12-12 12:23:18,209 [INFO] DeepIF: Epoch 11/50, Loss: 25.5518\n",
      "2024-12-12 12:23:18,225 [INFO] DeepIF: Epoch 12/50, Loss: 25.3646\n",
      "2024-12-12 12:23:18,241 [INFO] DeepIF: Epoch 13/50, Loss: 24.9755\n",
      "2024-12-12 12:23:18,256 [INFO] DeepIF: Epoch 14/50, Loss: 24.5616\n",
      "2024-12-12 12:23:18,272 [INFO] DeepIF: Epoch 15/50, Loss: 24.2889\n",
      "2024-12-12 12:23:18,287 [INFO] DeepIF: Epoch 16/50, Loss: 24.0544\n",
      "2024-12-12 12:23:18,302 [INFO] DeepIF: Epoch 17/50, Loss: 24.1332\n",
      "2024-12-12 12:23:18,318 [INFO] DeepIF: Epoch 18/50, Loss: 23.8885\n",
      "2024-12-12 12:23:18,368 [INFO] DeepIF: Epoch 19/50, Loss: 23.8133\n",
      "2024-12-12 12:23:18,392 [INFO] DeepIF: Epoch 20/50, Loss: 24.9321\n",
      "2024-12-12 12:23:18,409 [INFO] DeepIF: Epoch 21/50, Loss: 23.7008\n",
      "2024-12-12 12:23:18,424 [INFO] DeepIF: Epoch 22/50, Loss: 23.6489\n",
      "2024-12-12 12:23:18,440 [INFO] DeepIF: Epoch 23/50, Loss: 23.6210\n",
      "2024-12-12 12:23:18,456 [INFO] DeepIF: Epoch 24/50, Loss: 23.4196\n",
      "2024-12-12 12:23:18,471 [INFO] DeepIF: Epoch 25/50, Loss: 23.2703\n",
      "2024-12-12 12:23:18,487 [INFO] DeepIF: Epoch 26/50, Loss: 23.3213\n",
      "2024-12-12 12:23:18,503 [INFO] DeepIF: Epoch 27/50, Loss: 23.9805\n",
      "2024-12-12 12:23:18,518 [INFO] DeepIF: Epoch 28/50, Loss: 23.3789\n",
      "2024-12-12 12:23:18,533 [INFO] DeepIF: Epoch 29/50, Loss: 22.9418\n",
      "2024-12-12 12:23:18,548 [INFO] DeepIF: Epoch 30/50, Loss: 22.6570\n",
      "2024-12-12 12:23:18,564 [INFO] DeepIF: Epoch 31/50, Loss: 22.4586\n",
      "2024-12-12 12:23:18,579 [INFO] DeepIF: Epoch 32/50, Loss: 22.3493\n",
      "2024-12-12 12:23:18,593 [INFO] DeepIF: Epoch 33/50, Loss: 22.3148\n",
      "2024-12-12 12:23:18,608 [INFO] DeepIF: Epoch 34/50, Loss: 22.2791\n",
      "2024-12-12 12:23:18,623 [INFO] DeepIF: Epoch 35/50, Loss: 22.1140\n",
      "2024-12-12 12:23:18,639 [INFO] DeepIF: Epoch 36/50, Loss: 22.1426\n",
      "2024-12-12 12:23:18,656 [INFO] DeepIF: Epoch 37/50, Loss: 22.1674\n",
      "2024-12-12 12:23:18,671 [INFO] DeepIF: Epoch 38/50, Loss: 22.0724\n",
      "2024-12-12 12:23:18,686 [INFO] DeepIF: Epoch 39/50, Loss: 21.9396\n",
      "2024-12-12 12:23:18,701 [INFO] DeepIF: Epoch 40/50, Loss: 22.0701\n",
      "2024-12-12 12:23:18,717 [INFO] DeepIF: Epoch 41/50, Loss: 22.0784\n",
      "2024-12-12 12:23:18,732 [INFO] DeepIF: Epoch 42/50, Loss: 21.9089\n",
      "2024-12-12 12:23:18,747 [INFO] DeepIF: Epoch 43/50, Loss: 22.0496\n",
      "2024-12-12 12:23:18,763 [INFO] DeepIF: Epoch 44/50, Loss: 21.9733\n",
      "2024-12-12 12:23:18,778 [INFO] DeepIF: Epoch 45/50, Loss: 21.9723\n",
      "2024-12-12 12:23:18,794 [INFO] DeepIF: Epoch 46/50, Loss: 23.1430\n",
      "2024-12-12 12:23:18,809 [INFO] DeepIF: Epoch 47/50, Loss: 21.8826\n",
      "2024-12-12 12:23:18,825 [INFO] DeepIF: Epoch 48/50, Loss: 21.8821\n",
      "2024-12-12 12:23:18,841 [INFO] DeepIF: Epoch 49/50, Loss: 21.8650\n",
      "2024-12-12 12:23:18,856 [INFO] DeepIF: Epoch 50/50, Loss: 21.8942\n",
      "2024-12-12 12:23:18,972 [INFO] DeepIF: Detected 10 anomalies out of 1000 trades.\n",
      "2024-12-12 12:23:18,973 [ERROR] SHAP explainability failed: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray\n",
      "2024-12-12 12:23:18,974 [INFO] Combining anomaly detection results...\n",
      "2024-12-12 12:23:18,975 [INFO] Combined Results: 0 anomalies detected.\n",
      "2024-12-12 12:23:18,975 [INFO] Storing results...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DB_CONNECTION_STRING'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnomaly detection pipeline completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 62\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Optional: Explain anomalies (Top N)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Uncomment the following lines if you want to generate SHAP explanations\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# from interpretability import explain_anomalies\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# explain_anomalies(deepif_model, combined_df, preprocessor, top_n=10)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Store Results\u001b[39;00m\n\u001b[1;32m     61\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mstore_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnomaly detection pipeline completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m, in \u001b[0;36mstore_results\u001b[0;34m(df, config)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mStore anomaly detection results.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Choose storage method: database or CSV\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDB_CONNECTION_STRING\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDB_CONNECTION_STRING\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostgresql\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Store in database\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine\n\u001b[1;32m     13\u001b[0m     engine \u001b[38;5;241m=\u001b[39m create_engine(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDB_CONNECTION_STRING\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DB_CONNECTION_STRING'"
     ]
    }
   ],
   "source": [
    "# # main.py\n",
    "# import logging\n",
    "# import pandas as pd\n",
    "# from anomaly_config import CONFIG  # Updated import\n",
    "# from data_generation import generate_synthetic_data\n",
    "# from data_preprocessing import preprocess_data\n",
    "# from eif_model import run_eif\n",
    "# from deepif_model import run_deepif\n",
    "# from combine_results import combine_results\n",
    "# from store_results import store_results\n",
    "\n",
    "def main():\n",
    "    # Setup Logging\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, CONFIG['LOG_LEVEL']),\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s'\n",
    "    )\n",
    "    \n",
    "    # Generate Synthetic Data (Replace this with actual data loading if needed)\n",
    "    logging.info(\"Generating synthetic data...\")\n",
    "    data = generate_synthetic_data(size=CONFIG['SAMPLE_DATA_SIZE'])\n",
    "    \n",
    "    # Display first few rows of the synthetic data\n",
    "    print(data.head())\n",
    "    \n",
    "    # Preprocess Data\n",
    "    logging.info(\"Preprocessing data...\")\n",
    "    X_processed_df, preprocessor = preprocess_data(data, CONFIG['FEATURE_COLUMNS'])\n",
    "    \n",
    "    # Run Extended Isolation Forest (EIF) on preprocessed features\n",
    "    logging.info(\"Running Extended Isolation Forest (EIF)...\")\n",
    "    processed_eif_df = run_eif(\n",
    "        df=X_processed_df.copy(),\n",
    "        feature_columns=X_processed_df.columns.tolist(),\n",
    "        eif_params=CONFIG['EIF_PARAMS']\n",
    "    )\n",
    "    \n",
    "    # Run Deep Isolation Forest (DeepIF) on preprocessed features\n",
    "    logging.info(\"Running Deep Isolation Forest (DeepIF)...\")\n",
    "    processed_deepif_df, deepif_model = run_deepif(\n",
    "        df=X_processed_df.copy(),\n",
    "        feature_columns=X_processed_df.columns.tolist(),\n",
    "        deepif_params=CONFIG['DEEPIF_PARAMS'],\n",
    "        preprocessor=preprocessor\n",
    "    )\n",
    "    \n",
    "    # Combine Results\n",
    "    logging.info(\"Combining anomaly detection results...\")\n",
    "    combined_df = combine_results(\n",
    "        df_eif=processed_eif_df,\n",
    "        df_deepif=processed_deepif_df,\n",
    "        original_df=data\n",
    "    )\n",
    "    \n",
    "    # Optional: Explain anomalies (Top N)\n",
    "    # Uncomment the following lines if you want to generate SHAP explanations\n",
    "    # from interpretability import explain_anomalies\n",
    "    # explain_anomalies(deepif_model, combined_df, preprocessor, top_n=10)\n",
    "    \n",
    "    # Store Results\n",
    "    logging.info(\"Storing results...\")\n",
    "    store_results(combined_df, CONFIG)\n",
    "    \n",
    "    logging.info(\"Anomaly detection pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
